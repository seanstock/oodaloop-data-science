---
description: "oodaloop ds project rules"
globs: []
alwaysApply: true
---
## 1 Jupyter Notebook SQL Rules

### üö® CRITICAL: NO SQL IN NOTEBOOKS
**NEVER WRITE SQL directly in Jupyter notebook cells.** All SQL must be written in separate `.sql` files.

### ‚úÖ Required Workflow

1. Write SQL into separate files  
    - Save files in the `sql/` directory  
    - Use clear names like `revenue_by_channel.sql` or `sql/user_cohort_analysis.sql`  
    - Comment the purpose of each file  
2. Load SQL from file to run

Pattern A (direct):  
`df = client.query(load_sql('revenue_by_channel.sql')).to_dataframe()`

Pattern B (helper):  
`def run_sql(f): return client.query(load_sql(f)).to_dataframe()`  
`df = run_sql('revenue_by_channel.sql')`

### üéØ Why This Pattern?

- ‚úÖ SQL linting works (not possible inside notebooks)  
- ‚úÖ Easier diffs and versioning  
- ‚úÖ Compatible with testing and other tools  
- ‚úÖ Queries are reusable outside notebooks

---

## 2 Command Reference

| Goal                        | Command                                              | Notes                        |
|----------------------------|------------------------------------------------------|------------------------------|
| Initialise                 | `agent-redd-ds init`                                 | Use `--force` to overwrite   |
| Authenticate               | `agent-redd-ds login [--force]`                      | Populates local credentials  |
| List catalog               | `agent-redd-ds list-catalog [--format json\|table]`  | JSON for machine parsing     |
| List tables                | `agent-redd-ds list-tables <db>.<schema> [--format json\|table]` | Writes JSON metadata |
| Run query                  | `agent-redd-ds run-query <dir, or run-true>`         | Prints bytes scanned         |
| Validate SQL               | `agent-redd-ds validate-sql sql/your_file.sql`       | Checks queries for correctness |

---

## 3 Hard Requirements

- ‚ùå Do not write SQL into notebooks. Use `sql/` files and run `validate-sql` before execution.  
- ‚è≥ Do not start the analysis plan until the brief is reviewed and approved.

---

## 4 Best Practices

1. Reference JSON metadata instead of dumping schemas into chat.  
2. Use `.dry_run` for heavy queries and report cost.  
3. Respect token limits ‚Äî summarize where possible.  
4. Commit code, notebooks, docs, and versioned SQL in `sql/`; **never** store large extracts or credentials.

---

## 5 Anti-Patterns (‚ùå Avoid)

- Expanding whole datasets ‚Äújust in case‚Äù  
- Mutating BigQuery tables  
- Storing credentials or PII in the repo

---

## üìù Pre-Query Checklist

- [ ] Metadata exported for all referenced tables?  
- [ ] Preview run acceptable?
